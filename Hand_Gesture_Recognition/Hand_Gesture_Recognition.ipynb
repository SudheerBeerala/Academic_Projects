{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFYJggqa3jQ-"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tef4tDnq4crV",
    "outputId": "f695c287-9289-409f-9524-30da9773fd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLOvAyEeUkPH"
   },
   "outputs": [],
   "source": [
    "# !unzip \"/content/drive/MyDrive/ML_Projects/Project_data.zip\" -d \"/content/drive/MyDrive/ML_Projects/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tac5BWe9gka",
    "outputId": "39cb137b-d17e-4be3-ba46-670fb5aeccdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/ML_Projects/Project_data\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/ML_Projects/Project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8TloJH6t3jR6"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "# from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBsHBqoy3jSI"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5uDUEv_U3jSN"
   },
   "outputs": [],
   "source": [
    "# set seed value to get same results for every run\n",
    "\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "rn.seed(30)\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "# tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "La5KitDL3jST"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2I8HPZL53jSW"
   },
   "outputs": [],
   "source": [
    "# Define train, validation data set path files\n",
    "\n",
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1hmqrT63jS6"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "52nIIbEk3jS_",
    "outputId": "78f588a7-8124-4e16-c5f3-e704b6a33a0e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAIAAACTCYeWAAAzq0lEQVR4nG28WXMdV5Im6Ms5JyLuhosd4CKSIEWRklIplZSqzOosq6rusp6efpm36ccx6/5dYzbzOP04Xd1ttaTlWFYuVZOp3KhdXEGCINaLu0bEWdzn4VxQ6rIBjTAYEDfiLJ+7f/65n8D/9L/9LwAKAIKRAAAQkUCZABFIVQGWv0VE+B+/EFFBkUhFEFBVib69DAHqxaxp6tW1NSaTJBkEVQUgWV4iAoSgoAIIDAQAArr8BwCgBKKQVAFUAEAVARBAFJIkVaUkqACvxwnLn9W5IsXkvRcVIgFIAKSASvL68QbRAAiiACIBgpIqIRpQBQAiUtX/32ljnrwCiooCIhARIHx38kSESIgIqERESEkiABItLyJABFRFQCBAACAAAAV4/URFUAW5nBUiMiIAKiqJgAIoKCK9Htjy2QCz2cwY65xjywgCioAkkEAQEUTUEDGCAYhAQICgRgWIWCUhwOuZI6KqIoIqICIRgi4HuByOKgKoAr0eO17iCAARVBQIiEiXf8jfvkWKqNJy6/L//MWApBIxPxsQlBXzWmSkCiCIKGYAg6oqE8UUYxDDxrmOKCAIIAEAIyIDABoCA8IKCsiQ0aaohJr3AhS+g6gMfCIQAVVgANTlBPKw8HI1LvcMshVIUsOEJJpRq5iHrapIpCD5g6gK9J075NVRBiUERIUMlySKYIlIQRSEyADmzUdVBYQUExJLiCqAlq0p2hARKG+DXK40EBpVAlQAAWVQBSCUvBcKkIgoj3IJaUAkJBBEJP0W4cuFWU47b7wCIAEzMgFZcqoqkgBUkFAznoGUFVEyapAAKU8gTwYAURVB6BJ9ACCQUADyPiKAolw+DkARkBklCQJZVxCSiBDq8l6QPRkoKSIYIgHIz7vcLwAApMvHKQAR5fviJbIRAJEJKZvB5UcVABXyIwBBDbEhAwoioqLEpKIECCKAAEit98ZZYpIESQKCIhqABEgKpCIxBENs2fgUEcAQEgEREWISQQBF1bwKyx1bup+QEjNbZwGVM3RUlmACBVQkNISiqJCxl22DkABRM/QElHBpoop5VfJ8FVXgO2gHElQEzR8HQADDBrDNqyggCERIqggEgBCi2KrywS+mi7rxSaKIxCRtsxDR4cpwZTAAxXk7r6rSlgWBAqOkkP0fZMvPZpcdHy59BSL64Jmp40qRREiyHDRQEkFUAFE1qkkhmxMCY/YbuLwSAQx960dJVVEQEUH/ZQBAyLYJqMqE2ViYmQAlCRFJSogArL4J1hCxJYDDV0ePnz49OjprfFBNSSTF5H2rCoN+f3NjdWVlmGLsdauNjc1up6oKZ4iZCRGJnIqAClwafEYmAEiKROhcAZCxCa+dF+LSmymAQYY8UkTUS+eLALgMQgQgl66E8TKSLUMg6NLKETCbQF5HzZ9HRCLOjjc4Y0MKIlp2ypDkxcHB02fPHz55PpnNffzW3uhyNetmfHo6toVxRN2O6/cPhsPhlZ2t9dW1Xr9jjCFiBZUUsuOlDFBEVUkiREjEy6no8vaXIQMQABUMIKAiYsYQIOgStIgIjEv467duLa/E8pqlG8g8A78TZnE5DkQiA1iVnZRUSQVkPFvsvzj47MFnL1+Nki4nrJinAJAtUJdUpm2iB2iCv5jNZ/M5AcYgRXmdGfXSAwOmyxXTS+cjzIaZAGTJKRQy/0JEFcn8zLCaHGUQBV87cERV+g63IVy6PXjtdZePEcmURTO8CBSAs7sUkEQoyNaYojo5PDq7GC3a9uXh4f6zF7NZLd+yMoBLgvA//m7px0OAkHQ8mR2aU0Vz9fobCpRSZCZi0SV2X4dkSEmJMvkQygEaL6kKZJYCCGBUCRUJcen+BTAHaFIExcuYk5dzSXIISRXw0iuCAgDTJSKQstdXEGutta6ua0A6vbj42c9/2foYU0heMh16vWV4CfjMQjWbzndWRBXqJr06Po2Strc3bt28DpBAkIiyEWvGLaoERdWqKJiMaCSkpIJLeAKgvDYDgwBAeok7vVwlgYyqJYSBMjAUl+xd5fX+EC1Z/RJNl9wUEYG0bupurz+ezp4+fzGazFW+ned3N5kBHCEAeNH0OmBergsAiIIA+JiOT8+/+OorRLlx7QobK6JLa6cczDAkIQS+JMFIBElANfNbRdUl5NQAJkAFUMxBDEBykAVCMJkB5dkTggq9Nn5AQBRExqVNCSLB5V0QEInzA4Xoyy+/evJkP32HjnwX24RQGhp0OjHJeLZYWj4AAvDlz68vTiKHR0elsxsrK1WxmgO7Ll0vgUJKiXhp6DlFQqFLx5eRvMSTWXopJNFvOTwggi7d9nIoqpSUcBnZczBYbiEiEemlO309MRFh4sKVR0dHv//jg4vJLE/9X8RIQjAAhllEMIUSIQLIMnCDQ0CEIJAAZLlwGJp48up4NJpsrK0TgEKCS4eriqLCTKB0OR1UIlRc0jxIAgCoqmIQDGQ3ALT0B3nFEICAciqRd5zxknsCESgAAeFlrsNIrzliRj0hWkBCCKGNPuIlmL+LdgQgBWvAIoIPpLJSGUESpUx5CUVRUsKQtInJqyYBAZ0tGlUti9JHryCqoCoKmjNEYwxehiPVTLpEFXUZwvIQ2GS2m3c5IWejyBurIIwoQAhCCIKvozcCCivhkiTkKLLkCaAqKKRogKJvQdPmxmZv0D+fzv/lnmdgI5TMvaKo2BkmJGVkUFVRFAwQo4aYJClPF40Ej6AJIIogoQ9RRJEpDw8FkiZEJDQZnwYoSTZp1TwFfY1ONUSgKrp0ryKARJn/ECgDKCABosAlgUEgYFbN0wVlXE5dlBAUUQkUCFVTssTEHKKPksG23G1VsAAFQsc5Z7kwtrRV5YxhYkAWJRVESSBe0aOJCkoWC2xjaFQRQESROSaxZAASMkECAZWEjAYVSUkBCIQMJ1XNoBfIqEVUBTU5OC3pCl4mJQqkiAhCwIqSgZ7tWwEIFZjy5Jc5oIISAKmKkmC+JygAODYX56+mF+PXQQsVLELXmn5ZdK1zlqvCFWwYEBQJ0S7ZZdIEAygE0QvUSVR12vIiRAUAxkWIkdAagmWOgaAJQYwhRqAcsZSy/aISQN5/Xe4gqMlKS1aXMmVXVfrWbWG++jWzQSKA7/z5UpXJQU6yoWWajQQggJhibL3PS5QdeMfZXqfsVVWpUBZcldaRIQVVJFBAFZWkSVRIjcQUUkQ0RFqUDkMkBmQ6H19cTCbF2irnIWSNQCI7y8SIQJeDXupF2Z4vuT4imteTes1YBQSJQOCSu0rm7ai4NG54fV/Jd1gmiQDMpJnrABETMHFyvV6/VxWjWZt5lSWqnDOIwTdMKDXUixkBI5JhLgsLqElj8t63oW1CiBpFyVq1Dp1lg0mUgGLTOmJnjEhUFVVVUAVhslmtQyUBRc08BuAyG5Ml9VYDILo0YFRN2R+g5KihpHmamvM1QkQVoAyb7AQEstErKcql/sQIkP/EZNeGayu9wcXsJK+xM1S5IgY/a2sSEVERQAVD4AyXlTWIqCn42HgJAjHHfEkG0diysNYYunV7708/+nh9uEIKKrzMLgAUidmoJngd3/Pm6KWQpWm5WQAGCUCWNP71d7yUExlR9BLcef8JLjlCVoCQTVa0lrQiG7yIiGK7aIuiOj8dTaezJb1BMMaSM009a2NK6Vsa5wUWPrGkwrJEAdEc7YuKXacktEVR2dK1dbO9vfXWnb2tra3oa0UEAkISUVW1xhGhAuchEnBKSVWccRJDbJqqtIZdTLGVZJhIAFUyG0VVXcZBVQKmzGyX48PLRDfjAAiUkPKDl/Hgku4REQCWZeVcMRpdjOf1kskjsjFNCHX0YNBYRATI7lhVQRcJZimRgiMABcOAzpiidMb2er2iLE5Oz+azycsXB7s7O72qIxIRgYhiCCHEbqeDuEwLc/JaWJtiiG1rFZrZbHwRSuNsVdqqICQ2xryWgbLIp5oQEQkT6jLIASApXsqrORlmUrrk+oiChKqX2Q0s0/7ZbLqzs70xXFkSC0I24H2TIoiqgPgkUTXnHoxoACqm9X5lEQHAJ5gu/MV0OlpM575JoJ1e2Qbf63QtMogSYE4iRcQQMSIBZ6kICQWSSESVgjHUi8XowkZwlrNpk4jkCRARExMSZNKzlJzxktVcalVIhISIgqrIzJxFCwUGACJkZmZAVGMohJYZt3e2+v1+pvSOsWRklW5hOtZKhExcOYElYkCHsHd1+97tG8Ne0bXYc9QprGXj0EIEUuxVHYP01p07lXOaEmHOOhQA2BoyJudkhETKhhEBSDjU/uzopJktGKQ01hlrjDHee2vca1MHxZyzvhazvsNGL6WtHNbxMr0hQlX4trahAEqMRGAtMxs/nfe7VcZWyVAQdbtdQPQpkc6zCu4IDKIksYXZ6HV6xqwWRRcZjOGiAGZIYIHYp44tUtW11saUGEhVCDmpguYixDJlzTI5KDLzfDI9fPKsmc1KhMOXh0en5+vXdgdbmybFkFJagleW2q4AMCBSrshcUp8cvDUnPpc5PmJml6oBlmYGtAQHGEP1YiESPv74/a8fPU4xOWOM4dLYypXet4OiRCJGYElhUQffOmu6SH0yO93hKIwjMbMhRACllNJiXna7GzduFtaZwmGSJrRV4WKIImKNJQVBXeoDLIRmcnFx/PJgMZ9K6wMxF06RFDglNUlUJBpkTQKyzOcVAEh1WXyhzFdoqdMsCSpydgDClD2ZKiAxoS5zWyQkAEvmbHq2vrZhiJRStgtUkBgKZkUpC0eq0YsAAHC36BRoC3KDooRuOK4XbVN3yHS4VOZkWVDuvHVbIGFSViyMBdUQfIoJS1LDBAgKkgSUgoTDlwfz0Vlp7FxC4UxiVE1snOHCMHGMCRlfZ+6iy6Q3Vzkoox8RQJY0IXO4XIcjQmJSJCRkImJQzXUBVHTWihVrDJJ+753bn3/xjYqgoiQNMZJjZ5hR27aez6aYkK1JIocnR6dnZ5DEWcuOl1oiKAEmhRRiSNEUrrSOo7QCPqUk4gpXFE5AQTGGgKrG8uHhq8V02inL5L017KMvq6q/ulp1KiU03vskKRAzoGRRI/N8ynydEBQIiQhRKRczs0UhMRuyGEKUGFMISphrCpdCIXjmlHynKorCvPPO/W++epRCkiiskETROk1at7VINMZ5idP5zAcFUGtMvk9n2C+dZQUQBVBjDFpka03hYkoSveRIgWiYASElQVVLbJjbumkvJuBjnZIE3y3K1IZht7e1tW07lTo2zjlEtNYaspnhMxGgYo6UiKpES/k78VIIvBQ8CIkQSAUFDIKQKAgIyWWVRxRBiWkxX6SUDJEPoW1rV1YaRJO2rZforTODwWC+aBZNbFKbAECSQioYKyLDzihobFUAyCXCWbPo+drPmiIBICiRigpD40PUBDEVZMK8Pj088tNFZVyQFq11xqYETFQv5rO2Nt3SGGNCCARYWJYlP2EEVRTFLHoRA5ICsMkZfQ4NzMZYa6xFotB6iQmISXPO9C1ZlBTI+6pTra9sfr335aeffR1D0oKCtItmQQigYJGRjXWFsZawzXxWEJhh0dRWEdiQKgKWZUlFUZYdawpwqWQnIYokQGBmZGZAQEbRENNkMom1r0qXiEpboIISjSeTSAjOditnrLUxxZREZElNFfXSsyMjKSIp5t3P7BWWSrUykTWm9T6liAAgkhsDAClXo4FJBb33JNDr9j7+6AfHr07n04UXAeOiKoFaZ4Km45OTxqfWt1QYB6rZxgBDiBd+XBVlVZVE0LfuytWrq/1Br6y8qiWDrpjPZgUbV5ZCSIQoElt/Mho1iwWCTKZTNlQVVdvUkJIghBRIyTlniMiQjSKCmMunqkCca7KAwAjLJOdSz0XAnH0CMhqmsFRJs4XQUvxHUABJiZiccxITI9y+dfvPf/znv/j5rxrfFq6IMTEAiDQxLubzpk0RUWhZxAUEFXRWmzZ41dZgv3SmLFZWBqWzmiIhqyoSRwWHzGxSjISsMY7Ozh8/ekSiEoKk2OsPQkx16zGlbr8HqK5wRVkYRsMUQZMAZF2AKMc1fJ0DXlKcnAhhFjRyRkzGESeiBAqMmms0qkpISgigjEwAhIRKzaL58IMPnu/vf/r5l8hsDccUYoQYUhABhMIxW0sEmTiqoPgWDXmJGMPOoLeyvuaDz6tpmVU1xoSIOfkkUAZBxvOz0+lsurm2MffBlR1gM2taYrbGJgBF7vT7ruwQolrLxloRQSA2BpGXse1SdfqW2FKWfQwRM5O1JgsJzMs/AaLmVB4pJ0WAQES5tqciKaW9O3vIuGjrKCklkSQxeER0he2UZVU4oyQ+aRRD4FzR7fY6Zad0pWHLbLrdTuEKAmIiYo4pKigzgwIzK1Ibwny+KMuSiQTUFi6p+BSMtd1Bv9fvF2XBxiggIQIxMRtJSVUJeUkOVfGy3HuZvWTNWhFJQYmoKEpn3WWZWBQAkJAYkb6VybOOQAQAZVU2dXPjxq2927frpplMJnXTBu+XxUPi6FMzWcyni3rWNNNFM62XwjgZy1aS+tb3e30VZWQQ0ZRABRGstUSUF31Rt+ejUVmWMfiyKIEopKiaM3r1mtCSKwtTWBLMqRaJiIKAJiRFFCIgJMrMBsAAAi/LIIDCSMzW2hKQRTDFXFYQAjZZI2AUZsOWM8tHANYgsRr0yqr7zjvvdqoOICcVLzGoRtGYYoiiSo5tYYwzheFCBH0IPgU0REzW2E7VD8FnJq1JEMi4whiTOYghhgTz2bwsCx99URYxhhQjGyYmQY2SQkquclwwIRAqU5ZZU1QCRQQmyPIPERApkSz7loiZDTIoWVuxKWICVUYlS9YgIYmyKIIgITArGzWsRhIIEVmufc2Obty48fa9e0AkAJHQEyZrk7VUOi5LKBxYq0yRNYpGgRCDT5EMb+1sIalzRkHYOUGcziaDQVdVDKNhVE0htpV1JIhKTJR8BKBubyDAtihs4YqyJDaMWbdHBAU2RiTKMpHBDHEBoGXSiIYYciAQZGbrCmNM3bYAwLg0fVjKIJQt6rJPCRUEUBMqWxNB2Zr3/+SDw+Ojw8NXKUoCJRACSASsJDGpSE4vgNTk3IF5ZXVtc3tbFSCpJmmbBgiLqlJAImRjQ7MgBN+23W43qymqwMzOuqUaxSaqXrn+xtbWDrrC5AQeEEhJlSSJdQRAuXVjmZ7lzBVYMecuCGisdUwWABXAMDGiaFpW6bLAuayXIBAuywJIohqCX19bt4Ure902JRGNCoSKEiImBJSY+QJQAjZgmUJM71y78fZ73zfGgUI9mxrLpijms7k1DpEixjbGJFiW9vz0jNkgk3WWDBvnyNiYki1sUmAk58oYtfG1yQQeAIhVhJIIixrOilumVbmyAoiqQIScJBrHZbdix4Sca5LyWtwCAAZSImRQYWFQEkUgw8yhabtl96tvvv79Hx4cvzpadkkIJAVQTZr0siypACRgRZsU14YrDx8++ebh47/+q798+/694draZDYJUeo2loUzhEkAWXu9znwyWcwXxjICOlcwUVEUipQkOWNTlNXhcHfniuv2po03jK93yarEGKOqy+76UqcG1ct2TCQkIDTOFa4okI1kucoggUKCpR63VP5VVJgYCJVQgVLSqqqMtb/4+a9+8bs/rvUqZ2wIgXNjzWVooO/MHxGZaTQZE2AI8e9/8pPJdPbBe++trg7Oz8+ctd1utyw6ZCwjlNY8+eaVD6F0rq6bXqdDqt5HW7ioatmQ4e2dnW6/3wogGZNbGpDIMKtACAGXmiUSESzTOAUENjaJIBlQsFVZVt0YRQSJWBXIMICqJCZiY6JPzEyMUSISJpUE0O30QvCPn+6vDIfDytV1UzlXONu2QWXZ3iOXjQp5/pQSKxrGNgoDaILPvvhSRP7dX/+b7a3tJoZuUamkbtFVkYJpMp6GGAwREVpjSEESMKEpq05RJtSV4dBWVWhjaUpSYiVUBCQyxrAxsCxxkAIBMhM5Ywpn2DIAEjs0tuqtDFfXS1fFkJCtAklSNi6oRA3WcEreOqOqwMoFGeuKopw3zeHRyW8++U1/2Pvooz8htnPvfRJrTceakogBDCzFVAZEAEJlUBAEReNKdAUw/+HTz376s3+sfazKriAlJMPWoNFE81kT2iBR+t0eMQdJxlnLpt/vdqrKsmUy1pUK1Ov3DRBkKUJERMQwhxCcc86YGFJm6kTISDH4leHQRyFyq6trtiiMrV1lfcNGDUqM3q+tDOdN60NC5iRiXAEMZ4ej337yydlogkybW9vj6RzZGldcu7a7//LQew9MlbGghBoBJQGKaq6zRVGyEGJ0rloZrs7b9nQ8Cm17enpS9Tr//t//z4tFIyKgSgzzi8l0MiEiRIgxtq1HgKo0iGoNubIwVVn1+qpgbbGxsWleC5SGDVtOEqfTGQAYYyh36jJZy4aoMNY5J5DWNrffunevU3X24fFg0m/mOjs/vb67Mz4f1U07mzVFp2td2bR+fWP9d3/448GLFw8++zxE2dze4fF0bX1rf//5w8ePq6rcXFtrmqaZLdo2WUDLnN2s5u4wwYSaBFeGg7I3mC3ai9mk9X5QlrOm/cn/81MgqMquNUZjrArn5/Pzi3PDFAIbopSSM9YaJkZjDRNb65BM07araxtXr10zud1XAAiUQCXL3SJZmyQiumwrJjIpyfbWzocf/3D3+g0FsIV1RQc1/PPPf/bf/vbvet3+hz/4wd69dzd3dst+//Tw5d/83//l2f7zw8PDNop15cb2tg/x4NXRk2fPAGG+qFGgKstBbyW1PraecxUXU06qhdAaVuLh2lorMJoctd4z87xpOlVxNhr9/Oe/6A1WVOT0+Kjf6fQ7HZCEIN63TFRYi9ltEYmopGQcJ0lo2DjrnDOdTmeZuhu2bEKMnW4ny9pFWTKzQVUVALW2uHFj7/Y7761tXQEBBFjf2Nx/8uK//s1/+8NvfyMpvHx1+uTF4Y2be3fv3X/0+OGXn306GU92r+6cj8bD4Vq3tyICk8nk0eNHPgQ2xjAs6qZtfLcsS1vYgiFGy4AQc2OGECtTbzgIMR4enQCoZaOkRNbH6EXmdbt5pVdWpSKxpvl8ttZfyWTbp8jMZCAliTEZE7SSqnKOTVV1Vtc3y17HdHpdw0zM1hgiJkTnbCZ1xrJ1hUFU1RSTqG5s7aytb4OCqBLj468f/v3f/e356fnKyup8NpsvLjqDav/586fP9o+Pj0RkMOih4ZBSrzdAhk8//XRRN91uxxgTk6z2V2IbZ/PpvG5960trO0WJkpyt2FJMIQFG5KrsHZ+dheiJbVG4DMy2aY0x86YZjydlWd7a20u+efVsH1SznAOKklQE2hAY0SCQqmFeGa4Ohus716+xK00GBmYZDIXYqKqAoKD3AkRADAqSdNG255PJxmTSX10lxudP9n/yD/9wfnbKRDHE0AZjXbfbreu6bpqiKHq93ub2xie//W1ZdWrfTE5Pev3ejZs3DLvR+GJ0PlpdXTVEp6duOpmopkZiaheOCRSM6LyuuSg6vd50OpvOZr1ux1WFs242W5Cxg16vLKqmXmQaHkNIrZeoioqEChhTAgEAEhFnCAHER1UNIbY+mLJQUEMKuRIRAUySBFFVifMZG/A+okFmBjY+zJ88fjKZNu+89ycq+s///OvT0/Pjs5PFxaTX7W1d2T08fDmZTmOM0/nMOndld3dta73b7d6799Z8Pn/w4IG1dnxxUVU9w1xVVa/f73Q6wNY4G2Kom3k9nwUFoWjAiOGq22FnCWFrZ3s8Hle2GA6HK/1VZhoMBpubm7n1Yj6fTSbjtKjFByGKAswqIEHFlEVVuEGnZDIpKok5OT5fGHf7vfd6va6JKizAAMgkAJhEk4pRYMPMSWCZ6yAB0unJWRQOf3hQL+qDg5cnJyenr47ms/n52fnaxuq8qf1kUjhnrWPCVyfHyrCxsfHGGzfm8/nZ2XkMXlIi5kHZ1ZREYqfXrdsWCFWlqTvTsphOx5PGW5f6/Z7rdZwtU0ogIkkqVzCgD8GZ0re+bX2v00GDhgazqIs015jER3SGlJiZjKvKoqoKY60AIHFCHk3nF6+O//j5lx9++IEJ3iciwwYSsmJCZCaVBBiZGY1JITJHQg4+pqgnx+e156aenxyfPHn0+PTkFFRjiEHC+uYGMe/t7anq8dHRHx88OD5+ZYzZ3FhXkevXrxFAjCElKMui2+keH59YNhtrGz4EBGkW88319QRxOhsH733wZLhp28lkGmLY2dn+4PvfPz46KauOsZaZq063KF30rSmrkvh4sfDOWedUEyowcVkUhkgkjeezXgmDortoWzCc5vzpZ1/uXLtunLFElHVfo5B1qFx/QWMol+wACcAY1+0Wzw6OTDEYj+svv/r62bMnhsg5U3YKRJxMpnVTp5RWBitFUVy/dm1nc+349KRezA3D3s0bzIxIuc1jPJ5MZ/MU4+rqmrFOU4y+qZvalsYWfHJyevDioCjKw5PD4XBlfXOTmXuDwcXFhIhOT058CCvDYaeqVFOvrBgv+z5Fc88QswHR4FuRGJNait7H2XxKBqPQGpjzaWOywAYAKKLESIggCJR9fM7Io0hQZGuf77/wURazenbRvtw/EolFr9+pqkyKer1u8M3XX3y+MlxJKZ2dnc3euEZEbdtWVWdzY7MNng0389qn2B30D18dRx+HK/21tXVjCFSms0nT1t77F4uXlp0EQTYbW7v33nrr8y8+//KrbwybArHX67uiXB0OSbXxtS3c7GLU1o0lJhTVAAiqSYmjQgzAytG3It5amtftokWfTIgVJUmZ2CYRkZRSykKjSErJhyRRJGnKNcq69ca6+WLe7RRXru6sr69JiqJJNYmkKHF3d5uZt7e233vve7tXtvsr/f5Kny2Toeli6n2zmE1Pz07q+cwQbG6sWWsmk7lh7nQ7SVPVKYuiOD09BVBJoqC9TndyMfryiy+OXh7G1ktKmxtb1hbn5+f7z/dPTk8sU2k4NV5SQiQBZWYkEI0htj6Fuq5DTNY5AJ3P5jGJdQWZclYnI6qoklP6XHLIaqIq576TlBCRgGg8Gh8fn969e//p0/3V3ur3vvdWXW/PF7PTk9Oz8zNj+cXz/X6/r6CPHz+eL2bn5+dXru7me05n0xTj6uqqiizquQkmd0B1O53COUXwvp7NJnVdq8J0OlWBlZXhaHReFEWv2+uU5d7e3s7OztHR8eNHj/v9/pUrO71eT2OQ4Ceno4vTMw3JGktIy7MyiikCkLiiw2jH00mnVw1XVnzZb0zB1rKzJtccBYFVRRUANCFzPkFIkAiIRJGEY5R2URfWjC9Ozl4d3H/7rfX19bt37+zvP3/w6YOmaW/e3Pvmm69jjG3bXjVXy7L71Tff9LqdsqpOT06892+/8w4iHhwe1vWCiNvGr69urq2vt02dEgwG/aZpVSEGOT4+vnfv3vb2znQ229rcBID5fEFEW1ubw9VhvVgcvjo6PT3tlRUEf3F8knywhARKwJZJNEkCAUUmhaSE/a7z3h+fHl3E88Hurdlifn4xMqJ42aQoKkvJLSUVEsi1C0FEEySdHB8r6sHh8+liPDo929pdK2bm5OTY+1ZETk+OL0YX3W5fBKxxR0cnCORr3yA/ffR0NBqVVTk6Hc3m0+cHL2eTiWFbdjopQKfs1XVHNBYFM9NiMQ8hXIwnxrh7b93zEscX44ePHh4eHvZ7fWtt8eoQAYfDlWtXd1PTvnj8KNaLyhWFNUQsSoYpxmzOKaXEJnBBPvjpHB0Xg9W1bq8srQlNbXLzFACILk/pyeUxJYXEhlKMiAqY9vefGWueP3/W1HM2eHp6sr25HWJ48vjJaDRyrowxOWcm48mVK1fatmXDvvXWmpPp8ehisr6+fnR0NJ/XF6OLpm6qTmezP9jZ2kEEkTQanQPp+toaAhKRMWaxWDRtjcTT6fTVq1fWWgW9du3aoNdFBU1pdHh4+PxpB2i3P4gqiJgQGZHJKGsCiaIqigoxCXG3t77dIi7aUKqE2CQ/NYSooqqSNJ82Aco9RiIhtAZzENCUFDT1e0MgfP7s8cV4HNpw8Pwwxigim5ubqiqiMcWdnZ22bZ1ziFBWlXNF23oASDHNZgtVWOkNq8o7V6ytrd2/9xYRnZycvHx5EGMa9AZVVYlIjP7w8KAoLSi9Oj4ixB/+6EcvDw8NU2yb58+eVYYrwi5xmWBgbCOhlSSqgKgpoQJAShJBiRSccabsXswX3O+XZZEg+mY2n54bIJKkIPmcZO7pUgZVAkVKoAIJkdrYEuOint++82aM77548cKwmVxMi8Jtbm7evXvXOffw4WPv/fraxpMnT168ONjc3AAlY1xRdIgWKWldN87abrdTQdUfDG7cuLG9sz2+GDPz2vr6p59+Wtd7y6MyokdHR5PZtG4asubW7TvGGI3h6y8+cwjNdIrdbqdbrbrCKrGjAkwKkGKM4iVRUgkaVRITloXpdR2AByCJ85PjKVjeePe93krPtKHl3CROLJrykYCoiApJJYSgKgpycXHR6/WQTNs0u7u7KcnZydn6+pqq7u7uXr16tdvrTqfzr7/+GgBWVlaapiHilFKK0Vlrjcnm1e31QgjdTufe3be2N7eePX22v79/enZ6e+/2yclx07aqWjcNEgYfokxBcdhfwaS//sUvJMYSwUrs9/ppMZ/M56u9QdUtY4xNswDDTJgktbFVpSiSj+onCXU98y1xWUyno+5w49b1q4uLUbuoTdvUli0iQutziY1yDzFRTCIsEgMZHp2fqejO7tbZ+aiqihhj69vQtsbY09OT5y+6G+sbnapz88bN6Wz28OE3k8m0LItOp1OUpV62+vUHg7LbcSm9fe/+lStXvvj886fPni4WdV0vBv3B9ta2s3Y+n89mM2JmZuvc5uoGMcXJVGezflkUCgUbR4TdniFsF7WkZIxpWy8RE0ATY+sTACcRIWJnvcL6+sbxZHQxnvbWV1fWhmfHx4tZff3WbWPJOOcAIYmyKhDm8+TEbFgQrTKzsYWzlmE+nz59+vjKlavW2Lfv3z87O/W+HY3Onz/fr6rO2tpWt9sdDPr3377PzMdHx0XhrKUY1DnjCjdcWVlb39ze3l5fW33w4LMvPv/s1dFRr9u7fXsvSaqbpq7ruq6burbGls4NBv3t4Uo9nc6nk42yXCkrX9eFYWYKISYlNDxezI2xQJqixCQxZs1GIgiyi2xXhhud1Y00manFWRt5MttcvXLtylWLYAQhSMyNY8u3PqiCQoiR2aAIM4NoM2+PT88WTfvq6Pjw8OTK1d3V9eGt27dms2njm+Oz49H04vD4qNftF0Wxtb1x8/YbF5MzMnRydr69vV31OjHGqirX14fXru2+eH7wbP/Zom4YDUToVL1rb1z7/PNPF/VCEFxRptZbpA6SmU5Xgx8WzhkoU6yKQkBTSglS9F5FxNDUN4W11prQ+BgjMieNZEth2yQ5GF2cN401PJ8tTCrevr97c++mcawSTS4XEeVONFx2HiyPtVEmfG3btm1LxDGl3StXClOcjk7/4Sf/sDJcGY8vVPW9977f7/deHhxOxxMk3H+xLyJlpwwptt4ba4drawhQdjobmxvjyeTx08eT2azb7fa7AxFBpNW1td3dnVeHL32Ig25XiPqlG1guoxTEpiBkYCRACqqtb2MKbd5lYrBmVtcD7PQHvZPzU1ARVWtNd2W16PfJGjambeper3fv7fu7W1tJ4mg8994bVNCkiCAqiJcnsBRCiKBYFR2R1LbeWru21hnPZjs7O+vDNXiszAAAKytrVVm+cf2Num4m0wlbWswXMUlVVSfHxzliPz84cM71ej1F6Pb7Tx4/Pj49ni+mt27dWV9ZG41GrizW1oezyfr58WE9ma04w9ypCApQZjTGWoMqkkSjRJ9iE4IPUfIxv3z2TNPC1xULWVS2g6oLttp7883dq9fa4Efj8xcvXpRl0TSNSDo7PWtjsJaNiCJgSooMkPvmRQUQFQmQDSavklJVdCLi3u3b+/vPz05OhqvDRb04OjoybOp589//+9/PZtM//eHHb96587d/93fP9x/evXv3ytWd5/sHu7u7Dx8+LMrSWtupuvWifXH48uT0rNPp7N2+uba6bl/Yna0tTPHs5UvjfQ/BMfQ6pRWVmBiXJDOINo1vUgiSWh9VVAmBiBSjb60zMaXjs9NqsIJcuE5v7+691c3No+Pj6WLBuRbJ/Mb1m+OL6WQxu3n71nBtzbAxKSUfvEVLhKKyPBHMHCW1bRtC8N7HFMkWoOnum3cOnh+MLsb379/f29s7ODiYTqfD4XB3dzdG/7vf/VY1/at/9ePxeDKZjFNKbdt2ut0QvDNmc2tjOpscHx3H4K/d2bt6ZefZ02ezi/PDupaL0cn+Pqc4YOMISkRCzS3ESWTRhrqufQgRYHkelVFUUkoECoBJYgI13R4XpeuuDDd3VjY2nu0/f/D5l7Zwg0Gv0+lo0m+++Wp1fe3NO3e6vd7J4ZEhZyVAjAFjYs7V+Wzx6tvaN21RlrkSO5/Peyu9W7duDvsrv/ndJy+eP9+7feev/839yXT66tWrx48fPX36mA3dv3/vxz/+yy+//OLxo0fn56PpbPbG9TdG5xfD1XVnyqOjlxDTzubGrTeu1dPxs68+beaz81bCcG2IVLoCVQmFVRRREH2IPqS29XXroyQFBGbDJqaEAJpSG4OizJvadjrrWzuzeXPnrTevX7v5x08fPHvyPGlKbVMENzmejC/GP/j448FK79Xhy8HKMMZoLs7OkUABQus7nQ5yLhdEgUQgaI2PMSZVxNH4YlHX8+nio4//5Ifu45/+9KcH1t25ecetld98/fDRo8cA6e5bb7119/7pydnDh49EUhTgmFSgW/U21rdVYD6ZGUAErhCffPnAxMlaIR1nehg67FhVRaKkpNAIzL1v2hRjSjFGyR3BSIgJkmgkIAnJB59Qy053uL0zntdoi+HG5jdPHz18/LB0HVbt9/vEGGPa3NpOMT159PjWrVuxraezmcm9F/ngbxNqI8YaE1VQBVQdiBL5GOb1wjkjmi7G55999sd33/ne+++/99mDzz/57a8//PDD+/fuHh2/JKb3vvfe6urqz/7xZ6Pz883NzV6ne3x0sr25e/Xa9WtXr0/Gk05RTM7PfFM//Jyb2WhYdiyEkrggRomogKDJx9qnOkkT06xpY0iqKiqG2CAzUlJNSaJGQQSmNvmda1ds2fn66Yv+2vqDB188fvakKouN7W3rCkLwvq3rxWIx/fzzT2/t3To7P2t82+lU1Ol1yrIqipKYfUz5xEXu5mCmEAIoFFVZVqVz7s6d29vbW48ePvrHf/zZ/Xv3/uzP/uzw8OC3v/311tbG1atXALQoipTkq6++attmY3NjsNJPmtY3NpD5m0cPXx0ebm9uxKbeXV11MQ5MYSJYtYQmiQSRRlKtaZHipPWTul203sfYSggac79zktQG3/o2qix8qL2fNvXmzs72tWtfP3qyc+3qX/7rv9rZ3QkxEnFVlYNeVyRaa7xvz8/Ph6vDEPzTZ8+ssylGk0SWp+pVQbDxXlFBJMRkmJwtiElSSjENhyuTyeT+/fsX47ODg4Nf/PKXf/Hnf/ls/8Uf/vCg2+23rf/6q6+Pj056vX7TNMH7X//617vXdj/84Ydg9Pj8aO/WLasQ2/YH731/MTpD70vjUKKmlPLLsUSjJJ/ivPWL1vskQTSp5GMvKhKSGOMSSNt6NBxF6rbpr6/u3Xlz//lBArj/7v3V9ZUnz58i4sXFZDIZj0Znx0fHRVUZYwaDvvf+/Lzd3r3aLOqLixGF1ocQUorGGMscmtYHD2QUKQlG0Zg0JBWg8WR6dHTkffjg/Q/X1ja+/vrhb3//u/licjG9+OT3n/y/v/71eDyZzaa9Xud/+rf/7tatvelkOh6NfvDRn0ymF1HavVs33n7rzfFoNOj2HGCHbNcUDCop1E07rf100c7qZlo387bx0YcYgw8pCSigMgAlhTa0s3pRh1AHv2jr4cb6O++/P5rOnh88f/e9d5T573/6k8Pjo92d7V63AsLR5GJ0MfaNR6AUdTKeIpq2aY+OjjudDuVjtAKQUiRDyBxCjCmSMYLQpthEz4a73aqu5xeT0S9+9YuQ/PODF0cnR8cnx8bafq8/XyzuvXX3P/6n//j+++8bY0Tiu+++++Mf/9naytAS//DjH4Lg73/3u6ZpruxeefTkcUgJjWmS+AQ+atsmX/vFopnVYdH6ugmLps0vNNMUJSXv2xB8ktSE4FU8SiuChXvng/dDSi9evbp1+87OlSvfPHo4mc3/9V/+1d27b25f2TbOdjt954p+f2U+m8/rRbffN0V5Pr7oDQbGWAoxhhCiRM3HQwjrulks6saHKCCCMaQUQ5JAhkOIf3zwx//9//g/Hz55dOv27fvvvHP46tXuld1333nnL/7iLxDg0aNHq2trKca1tdVOp7M2XP3tbz5Rkf/wv/6HQX/wi1/8fDafff+DD6peb1ovxrNZG5NPEpO2TVM3ftH6RRPqpmlDCCnCJeEEhda3bWiiCrCZt21CuPvO/arff/HqSMm8ef+d5wevjo9O7r95bzabPXz82Bg7HA77vT4ihRCtcyK6aJuz0cg4h8zT6dyoJkVQwVaUNKUQEElEfRMsOzaYYlRRa9zacP3B7x+kIAcvD4erK4bt/rMX/W7vxhs3tra2Hj96+oc//u6jjz7a29v75a9+9e73vue973c6bM3vf/+7R48fX79y9ea1q8++eehnk/ls7JsFE2FKbVvHKMmrTz6iJtUIKqAmn82X3DFAotCGCAbb2JS9XlVVu9feePxs//j07J333l/U/tE3j7Y3dm7d2PvVr355fjbqvtFv6zCZzlISYw0Fc342alPavXLVWvfq1dHmcM28fgEACimIs64oiIgNWgKKKrnb0MfwySefjCeToiyuXr3y1lt3375//+nT/U6/N1vM+WL0m08+6XbLtbW1qqqqsvy//vN/3trcZISbezd/9KMfPnm6//DRY0e4s7FmUM8OD5FJAZratz4sD0hIyuf7yVhSzm+rUSXvQxQVosTcxFh1ez/4+E/z+8tOz0cbG7vDlbVPfvNb69zt27eePn0yvpjcuXPHOTudTNv5olMWZVnWdU3MxtjM8FW1KEuT5cvLo6L6+vSooiZNCZSJo8T9/acvD1/e3LsJAG+9dXc6nX311VeTyfRiMr56dXf/+fOPPvrw0aOHn3zyyZWrV2/eunUxmYwn45OjV1989WV/sLJ75fobN984OTw8GY3qi4uE4H2rkmLwjW9STCr5xP3lYW5CVY2SYko+SVJtvSTUwera2+++uzJcjSrPXzyPoh9/8P2Hjx+1wb9x49ro4uzLL78pi45x9vT8tGlqRXDOSUooUFWlMWY0GnW7/W6382x/36QU8otz2DgGUlAB1ZiElJhEBRmn08mzgycf/emHxhpjrBqqQwOkk8nF1d1dw+ZHf/pDa+3W9sY//dM/f/nll0+fPNna3q6K4uzkuF4sfvS992bzxX/5r39z9/ZtSOnujRtPv/piPJ06Z4L3IcUQWgImsMS5bxGUQVRSlEXjE5CzhcbQqbq3bu5dv3r9l//0y/7q6mw+v33nTSE5HZ3YygDD108ejmcX61ubzw6enp+fdbu9btnrdLpVVZyd+PF4StaWncJaO5nMIMr/B+9NPzpjdByOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x84 at 0x7F1677C53F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (84, 84, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if image cropping works okay\n",
    "\n",
    "train_path = 'train'\n",
    "image = Image.open(train_path+'/'+ 'WIN_20180926_18_09_50_Pro_Right_Swipe_new' +'/'+'WIN_20180926_18_09_50_Pro_00013.png')\n",
    "image = Image.fromarray(np.array(image)[:,20:140,:]).resize((84,84))\n",
    "display(image)\n",
    "image = np.array(image).astype(np.float32)\n",
    "type(image), image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "8THspyN03jTN",
    "outputId": "12894cc1-f524-4ece-f74a-68b8bfc376e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x84 at 0x7F1677CCBF90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (84, 84, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if image cropping works okay\n",
    "\n",
    "image = Image.open(train_path+'/'+ 'WIN_20180907_15_35_09_Pro_Right Swipe_new' +'/'+'WIN_20180907_15_35_09_Pro_00012.png')\n",
    "image = image.resize((84,84))\n",
    "display(image)\n",
    "image = np.array(image).astype(np.float32)\n",
    "type(image), image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGO5UX4F3jTW"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    # Select 18 images from the available 30\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,18,84,84,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if np.array(image).shape[1] == 160:\n",
    "                        image = Image.fromarray(np.array(image)[:,20:140,:]).resize((84,84))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = image.resize((84,84))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image,(84,84)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,18,84,84,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    # image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if np.array(image).shape[1] == 160:\n",
    "                        image = Image.fromarray(np.array(image)[:,20:140,:]).resize((84,84))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image[:,20:140,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = image.resize((84,84))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUJUojeB3jTx"
   },
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luFqTumf3jT3",
    "outputId": "e495b49c-de54-48ac-cc30-005b32c30dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation data paths\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y58drEm03jT6"
   },
   "source": [
    "## Model - 1\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6qOomJA3jT9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQZGOmH93jUA"
   },
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_pxmv393jUC",
    "outputId": "097de131-eda1-4a55-afd3-ae9888d04625",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 18, 84, 84, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 18, 84, 84, 64)   256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 18, 84, 84, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 9, 42, 84, 64)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 9, 42, 84, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 9, 42, 84, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 42, 84, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 4, 21, 42, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 4, 21, 42, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 21, 42, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 4, 21, 42, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 10, 21, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 10, 21, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 10, 21, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 10, 21, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 5, 10, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6554112   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,440,773\n",
      "Trainable params: 9,439,365\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and compile the model\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LohPiEMa3jUI"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfOKSiNA3jUK"
   },
   "outputs": [],
   "source": [
    "# Define train, validation data generators\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0x0MsPZ3jUM",
    "outputId": "e9fa2922-b2b8-44a5-c1a2-d0b4acf28cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "# Save model during callback\n",
    "\n",
    "model_name = 'Conv3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq2uc-b23jUP"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBsjvkkN3jUR"
   },
   "outputs": [],
   "source": [
    "# define steps for epoch and validation\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sReh6_Ne3jUv"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKtXMaoP3jUw",
    "outputId": "250e86dc-107d-4792-c188-fb0a02e50b01",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 39\n",
      "Epoch 1/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.6048 - categorical_accuracy: 0.3243  Source path =  val ; batch size = 39\n",
      "\n",
      "Epoch 00001: saving model to Conv3D_2022-02-1314_31_21.932105/model-00001-3.60477-0.32428-10.81863-0.23000.h5\n",
      "17/17 [==============================] - 7229s 451s/step - loss: 3.6048 - categorical_accuracy: 0.3243 - val_loss: 10.8186 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8053 - categorical_accuracy: 0.5038\n",
      "Epoch 00002: saving model to Conv3D_2022-02-1314_31_21.932105/model-00002-1.80530-0.50377-6.43797-0.23000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.8053 - categorical_accuracy: 0.5038 - val_loss: 6.4380 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5384 - categorical_accuracy: 0.5294\n",
      "Epoch 00003: saving model to Conv3D_2022-02-1314_31_21.932105/model-00003-1.53844-0.52941-6.73693-0.24000.h5\n",
      "17/17 [==============================] - 75s 5s/step - loss: 1.5384 - categorical_accuracy: 0.5294 - val_loss: 6.7369 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2994 - categorical_accuracy: 0.5505\n",
      "Epoch 00004: saving model to Conv3D_2022-02-1314_31_21.932105/model-00004-1.29939-0.55053-5.36193-0.23000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.2994 - categorical_accuracy: 0.5505 - val_loss: 5.3619 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0993 - categorical_accuracy: 0.6214\n",
      "Epoch 00005: saving model to Conv3D_2022-02-1314_31_21.932105/model-00005-1.09926-0.62142-4.28931-0.25000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.0993 - categorical_accuracy: 0.6214 - val_loss: 4.2893 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0338 - categorical_accuracy: 0.6501\n",
      "Epoch 00006: saving model to Conv3D_2022-02-1314_31_21.932105/model-00006-1.03375-0.65008-2.98829-0.34000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.0338 - categorical_accuracy: 0.6501 - val_loss: 2.9883 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9572 - categorical_accuracy: 0.6833\n",
      "Epoch 00007: saving model to Conv3D_2022-02-1314_31_21.932105/model-00007-0.95721-0.68326-1.65380-0.43000.h5\n",
      "17/17 [==============================] - 78s 5s/step - loss: 0.9572 - categorical_accuracy: 0.6833 - val_loss: 1.6538 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9425 - categorical_accuracy: 0.6787\n",
      "Epoch 00008: saving model to Conv3D_2022-02-1314_31_21.932105/model-00008-0.94246-0.67873-1.28939-0.56000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.9425 - categorical_accuracy: 0.6787 - val_loss: 1.2894 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7280 - categorical_accuracy: 0.7436\n",
      "Epoch 00009: saving model to Conv3D_2022-02-1314_31_21.932105/model-00009-0.72800-0.74359-0.82136-0.68000.h5\n",
      "17/17 [==============================] - 84s 5s/step - loss: 0.7280 - categorical_accuracy: 0.7436 - val_loss: 0.8214 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7654 - categorical_accuracy: 0.7315\n",
      "Epoch 00010: saving model to Conv3D_2022-02-1314_31_21.932105/model-00010-0.76536-0.73152-0.94634-0.66000.h5\n",
      "17/17 [==============================] - 78s 5s/step - loss: 0.7654 - categorical_accuracy: 0.7315 - val_loss: 0.9463 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7011 - categorical_accuracy: 0.7466\n",
      "Epoch 00011: saving model to Conv3D_2022-02-1314_31_21.932105/model-00011-0.70111-0.74661-0.68820-0.77000.h5\n",
      "17/17 [==============================] - 78s 5s/step - loss: 0.7011 - categorical_accuracy: 0.7466 - val_loss: 0.6882 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6719 - categorical_accuracy: 0.7662\n",
      "Epoch 00012: saving model to Conv3D_2022-02-1314_31_21.932105/model-00012-0.67195-0.76621-0.67040-0.68000.h5\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.6719 - categorical_accuracy: 0.7662 - val_loss: 0.6704 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5544 - categorical_accuracy: 0.7873\n",
      "Epoch 00013: saving model to Conv3D_2022-02-1314_31_21.932105/model-00013-0.55443-0.78733-0.68181-0.77000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.5544 - categorical_accuracy: 0.7873 - val_loss: 0.6818 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5290 - categorical_accuracy: 0.8084\n",
      "Epoch 00014: saving model to Conv3D_2022-02-1314_31_21.932105/model-00014-0.52901-0.80845-0.57508-0.82000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.5290 - categorical_accuracy: 0.8084 - val_loss: 0.5751 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4781 - categorical_accuracy: 0.8115\n",
      "Epoch 00015: saving model to Conv3D_2022-02-1314_31_21.932105/model-00015-0.47811-0.81146-0.72429-0.73000.h5\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.4781 - categorical_accuracy: 0.8115 - val_loss: 0.7243 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4918 - categorical_accuracy: 0.8039\n",
      "Epoch 00016: saving model to Conv3D_2022-02-1314_31_21.932105/model-00016-0.49184-0.80392-0.63201-0.78000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.4918 - categorical_accuracy: 0.8039 - val_loss: 0.6320 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5027 - categorical_accuracy: 0.8296\n",
      "Epoch 00017: saving model to Conv3D_2022-02-1314_31_21.932105/model-00017-0.50273-0.82956-0.65731-0.73000.h5\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.5027 - categorical_accuracy: 0.8296 - val_loss: 0.6573 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3460 - categorical_accuracy: 0.8748\n",
      "Epoch 00018: saving model to Conv3D_2022-02-1314_31_21.932105/model-00018-0.34604-0.87481-0.66302-0.79000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.3460 - categorical_accuracy: 0.8748 - val_loss: 0.6630 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2963 - categorical_accuracy: 0.8854\n",
      "Epoch 00019: saving model to Conv3D_2022-02-1314_31_21.932105/model-00019-0.29625-0.88537-0.68104-0.79000.h5\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.2963 - categorical_accuracy: 0.8854 - val_loss: 0.6810 - val_categorical_accuracy: 0.7900 - lr: 2.5000e-04\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3419 - categorical_accuracy: 0.8673\n",
      "Epoch 00020: saving model to Conv3D_2022-02-1314_31_21.932105/model-00020-0.34188-0.86727-0.76239-0.75000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.3419 - categorical_accuracy: 0.8673 - val_loss: 0.7624 - val_categorical_accuracy: 0.7500 - lr: 2.5000e-04\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2909 - categorical_accuracy: 0.8854\n",
      "Epoch 00021: saving model to Conv3D_2022-02-1314_31_21.932105/model-00021-0.29090-0.88537-0.67570-0.78000.h5\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.2909 - categorical_accuracy: 0.8854 - val_loss: 0.6757 - val_categorical_accuracy: 0.7800 - lr: 1.2500e-04\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2928 - categorical_accuracy: 0.8884\n",
      "Epoch 00022: saving model to Conv3D_2022-02-1314_31_21.932105/model-00022-0.29278-0.88839-0.62249-0.80000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.2928 - categorical_accuracy: 0.8884 - val_loss: 0.6225 - val_categorical_accuracy: 0.8000 - lr: 1.2500e-04\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2913 - categorical_accuracy: 0.8914\n",
      "Epoch 00023: saving model to Conv3D_2022-02-1314_31_21.932105/model-00023-0.29128-0.89140-0.54654-0.82000.h5\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.2913 - categorical_accuracy: 0.8914 - val_loss: 0.5465 - val_categorical_accuracy: 0.8200 - lr: 6.2500e-05\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2982 - categorical_accuracy: 0.8944\n",
      "Epoch 00024: saving model to Conv3D_2022-02-1314_31_21.932105/model-00024-0.29823-0.89442-0.66120-0.74000.h5\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.2982 - categorical_accuracy: 0.8944 - val_loss: 0.6612 - val_categorical_accuracy: 0.7400 - lr: 6.2500e-05\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2806 - categorical_accuracy: 0.8944\n",
      "Epoch 00025: saving model to Conv3D_2022-02-1314_31_21.932105/model-00025-0.28064-0.89442-0.60303-0.78000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.2806 - categorical_accuracy: 0.8944 - val_loss: 0.6030 - val_categorical_accuracy: 0.7800 - lr: 6.2500e-05\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3104 - categorical_accuracy: 0.8793\n",
      "Epoch 00026: saving model to Conv3D_2022-02-1314_31_21.932105/model-00026-0.31043-0.87934-0.67686-0.76000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.3104 - categorical_accuracy: 0.8793 - val_loss: 0.6769 - val_categorical_accuracy: 0.7600 - lr: 3.1250e-05\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2919 - categorical_accuracy: 0.8884\n",
      "Epoch 00027: saving model to Conv3D_2022-02-1314_31_21.932105/model-00027-0.29192-0.88839-0.63272-0.76000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.2919 - categorical_accuracy: 0.8884 - val_loss: 0.6327 - val_categorical_accuracy: 0.7600 - lr: 3.1250e-05\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3032 - categorical_accuracy: 0.8869\n",
      "Epoch 00028: saving model to Conv3D_2022-02-1314_31_21.932105/model-00028-0.30319-0.88688-0.58490-0.78000.h5\n",
      "17/17 [==============================] - 78s 5s/step - loss: 0.3032 - categorical_accuracy: 0.8869 - val_loss: 0.5849 - val_categorical_accuracy: 0.7800 - lr: 1.5625e-05\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3150 - categorical_accuracy: 0.8854\n",
      "Epoch 00029: saving model to Conv3D_2022-02-1314_31_21.932105/model-00029-0.31497-0.88537-0.55183-0.81000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.3150 - categorical_accuracy: 0.8854 - val_loss: 0.5518 - val_categorical_accuracy: 0.8100 - lr: 1.5625e-05\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2503 - categorical_accuracy: 0.9050\n",
      "Epoch 00030: saving model to Conv3D_2022-02-1314_31_21.932105/model-00030-0.25032-0.90498-0.58216-0.78000.h5\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.2503 - categorical_accuracy: 0.9050 - val_loss: 0.5822 - val_categorical_accuracy: 0.7800 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f16702bd150>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUzXR0053jUz"
   },
   "source": [
    "## Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vZG_OvH73jU2"
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1Zndb0-53jU4"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,15,120,120,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                  # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if np.array(image).shape[1] == 160:\n",
    "                        image = Image.fromarray(np.array(image)[:,20:140,:])\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = image[:,20:140,:].astype(np.float32)\n",
    "                    else:\n",
    "                        image = image.resize((120,120))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image,(120,120)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,15,120,120,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    # image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    if np.array(image).shape[1] == 160:\n",
    "                        image = Image.fromarray(np.array(image)[:,20:140,:])\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = image[:,20:140,:].astype(np.float32)\n",
    "                    else:\n",
    "                        image = image.resize((120,120))\n",
    "                        image = np.array(image).astype(np.float32)\n",
    "                        # image = imresize(image,(120,120)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fLlsyk83jU8",
    "outputId": "79d00ac9-2548-4734-d597-7a537b68076d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 70\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 70\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kioBlOXD3jU_",
    "outputId": "f51987d6-b23a-4762-d7af-d8b8bd594879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "    \n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(120,120,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x.add(Dropout(0.5))\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "    \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(conv_model, input_shape=(15,120,120,3)))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rbtzebdp3jVI",
    "outputId": "af04779b-2bd4-46c2-967d-4a218ca82510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 15, 64)           15009664  \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 15, 32)            9408      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 16)                2400      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,021,653\n",
      "Trainable params: 306,965\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "j6imA3Ic3jVK"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpghHi6J3jVM",
    "outputId": "11cc4b07-f5e9-48cc-fac2-ae828bae6479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Conv_GRU' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ni08pDOB3jVO"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKZRZSHY3jVQ",
    "outputId": "729a54f5-8752-4497-9aba-96537516f814",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 17\n",
      "Epoch 1/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.6379 - categorical_accuracy: 0.2006Source path =  val ; batch size = 17\n",
      "\n",
      "Epoch 00001: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00001-1.63791-0.20060-1.58593-0.30000.h5\n",
      "39/39 [==============================] - 124s 2s/step - loss: 1.6379 - categorical_accuracy: 0.2006 - val_loss: 1.5859 - val_categorical_accuracy: 0.3000 - lr: 0.0100\n",
      "Epoch 2/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.5410 - categorical_accuracy: 0.2775\n",
      "Epoch 00002: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00002-1.54096-0.27753-1.41821-0.37000.h5\n",
      "39/39 [==============================] - 65s 2s/step - loss: 1.5410 - categorical_accuracy: 0.2775 - val_loss: 1.4182 - val_categorical_accuracy: 0.3700 - lr: 0.0100\n",
      "Epoch 3/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.4335 - categorical_accuracy: 0.3424\n",
      "Epoch 00003: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00003-1.43346-0.34238-1.32568-0.40000.h5\n",
      "39/39 [==============================] - 66s 2s/step - loss: 1.4335 - categorical_accuracy: 0.3424 - val_loss: 1.3257 - val_categorical_accuracy: 0.4000 - lr: 0.0100\n",
      "Epoch 4/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.2807 - categorical_accuracy: 0.3590\n",
      "Epoch 00004: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00004-1.28069-0.35897-1.20483-0.37000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 1.2807 - categorical_accuracy: 0.3590 - val_loss: 1.2048 - val_categorical_accuracy: 0.3700 - lr: 0.0100\n",
      "Epoch 5/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.1932 - categorical_accuracy: 0.4223\n",
      "Epoch 00005: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00005-1.19320-0.42232-1.03053-0.47000.h5\n",
      "39/39 [==============================] - 65s 2s/step - loss: 1.1932 - categorical_accuracy: 0.4223 - val_loss: 1.0305 - val_categorical_accuracy: 0.4700 - lr: 0.0100\n",
      "Epoch 6/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.1547 - categorical_accuracy: 0.4118\n",
      "Epoch 00006: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00006-1.15475-0.41176-1.16452-0.44000.h5\n",
      "39/39 [==============================] - 70s 2s/step - loss: 1.1547 - categorical_accuracy: 0.4118 - val_loss: 1.1645 - val_categorical_accuracy: 0.4400 - lr: 0.0100\n",
      "Epoch 7/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.1810 - categorical_accuracy: 0.4012\n",
      "Epoch 00007: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00007-1.18095-0.40121-1.18957-0.41000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "39/39 [==============================] - 65s 2s/step - loss: 1.1810 - categorical_accuracy: 0.4012 - val_loss: 1.1896 - val_categorical_accuracy: 0.4100 - lr: 0.0100\n",
      "Epoch 8/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.1118 - categorical_accuracy: 0.4419\n",
      "Epoch 00008: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00008-1.11182-0.44193-1.05575-0.51000.h5\n",
      "39/39 [==============================] - 66s 2s/step - loss: 1.1118 - categorical_accuracy: 0.4419 - val_loss: 1.0558 - val_categorical_accuracy: 0.5100 - lr: 0.0070\n",
      "Epoch 9/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.0665 - categorical_accuracy: 0.4480\n",
      "Epoch 00009: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00009-1.06652-0.44796-1.01188-0.53000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 1.0665 - categorical_accuracy: 0.4480 - val_loss: 1.0119 - val_categorical_accuracy: 0.5300 - lr: 0.0070\n",
      "Epoch 10/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.0309 - categorical_accuracy: 0.4751\n",
      "Epoch 00010: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00010-1.03090-0.47511-1.03132-0.42000.h5\n",
      "39/39 [==============================] - 66s 2s/step - loss: 1.0309 - categorical_accuracy: 0.4751 - val_loss: 1.0313 - val_categorical_accuracy: 0.4200 - lr: 0.0070\n",
      "Epoch 11/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.0025 - categorical_accuracy: 0.4676\n",
      "Epoch 00011: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00011-1.00246-0.46757-0.94061-0.50000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 1.0025 - categorical_accuracy: 0.4676 - val_loss: 0.9406 - val_categorical_accuracy: 0.5000 - lr: 0.0070\n",
      "Epoch 12/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.9772 - categorical_accuracy: 0.4947\n",
      "Epoch 00012: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00012-0.97715-0.49472-0.90179-0.65000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.9772 - categorical_accuracy: 0.4947 - val_loss: 0.9018 - val_categorical_accuracy: 0.6500 - lr: 0.0070\n",
      "Epoch 13/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.9182 - categorical_accuracy: 0.5264\n",
      "Epoch 00013: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00013-0.91821-0.52640-0.91024-0.63000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.9182 - categorical_accuracy: 0.5264 - val_loss: 0.9102 - val_categorical_accuracy: 0.6300 - lr: 0.0070\n",
      "Epoch 14/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.9020 - categorical_accuracy: 0.5385\n",
      "Epoch 00014: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00014-0.90201-0.53846-0.85787-0.57000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.9020 - categorical_accuracy: 0.5385 - val_loss: 0.8579 - val_categorical_accuracy: 0.5700 - lr: 0.0070\n",
      "Epoch 15/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8875 - categorical_accuracy: 0.5294\n",
      "Epoch 00015: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00015-0.88750-0.52941-0.89812-0.63000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.8875 - categorical_accuracy: 0.5294 - val_loss: 0.8981 - val_categorical_accuracy: 0.6300 - lr: 0.0070\n",
      "Epoch 16/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7957 - categorical_accuracy: 0.6184\n",
      "Epoch 00016: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00016-0.79575-0.61840-0.88266-0.59000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.7957 - categorical_accuracy: 0.6184 - val_loss: 0.8827 - val_categorical_accuracy: 0.5900 - lr: 0.0070\n",
      "Epoch 17/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7501 - categorical_accuracy: 0.6863\n",
      "Epoch 00017: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00017-0.75006-0.68627-0.73379-0.69000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.7501 - categorical_accuracy: 0.6863 - val_loss: 0.7338 - val_categorical_accuracy: 0.6900 - lr: 0.0049\n",
      "Epoch 18/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6845 - categorical_accuracy: 0.6863\n",
      "Epoch 00018: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00018-0.68451-0.68627-0.69419-0.71000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.6845 - categorical_accuracy: 0.6863 - val_loss: 0.6942 - val_categorical_accuracy: 0.7100 - lr: 0.0049\n",
      "Epoch 19/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6544 - categorical_accuracy: 0.7014\n",
      "Epoch 00019: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00019-0.65442-0.70136-0.65369-0.71000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.6544 - categorical_accuracy: 0.7014 - val_loss: 0.6537 - val_categorical_accuracy: 0.7100 - lr: 0.0049\n",
      "Epoch 20/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6340 - categorical_accuracy: 0.7059\n",
      "Epoch 00020: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00020-0.63398-0.70588-0.59685-0.76000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.6340 - categorical_accuracy: 0.7059 - val_loss: 0.5969 - val_categorical_accuracy: 0.7600 - lr: 0.0049\n",
      "Epoch 21/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5883 - categorical_accuracy: 0.7481\n",
      "Epoch 00021: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00021-0.58826-0.74811-0.67833-0.75000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.5883 - categorical_accuracy: 0.7481 - val_loss: 0.6783 - val_categorical_accuracy: 0.7500 - lr: 0.0049\n",
      "Epoch 22/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5855 - categorical_accuracy: 0.7466\n",
      "Epoch 00022: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00022-0.58547-0.74661-0.69895-0.75000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0034300000406801696.\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.5855 - categorical_accuracy: 0.7466 - val_loss: 0.6990 - val_categorical_accuracy: 0.7500 - lr: 0.0049\n",
      "Epoch 23/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5048 - categorical_accuracy: 0.8069\n",
      "Epoch 00023: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00023-0.50478-0.80694-0.51715-0.84000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.5048 - categorical_accuracy: 0.8069 - val_loss: 0.5172 - val_categorical_accuracy: 0.8400 - lr: 0.0034\n",
      "Epoch 24/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4739 - categorical_accuracy: 0.8356\n",
      "Epoch 00024: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00024-0.47391-0.83560-0.56024-0.76000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.4739 - categorical_accuracy: 0.8356 - val_loss: 0.5602 - val_categorical_accuracy: 0.7600 - lr: 0.0034\n",
      "Epoch 25/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4360 - categorical_accuracy: 0.8492\n",
      "Epoch 00025: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00025-0.43597-0.84917-0.48035-0.80000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.4360 - categorical_accuracy: 0.8492 - val_loss: 0.4804 - val_categorical_accuracy: 0.8000 - lr: 0.0034\n",
      "Epoch 26/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4631 - categorical_accuracy: 0.8220\n",
      "Epoch 00026: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00026-0.46307-0.82202-0.49215-0.86000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.4631 - categorical_accuracy: 0.8220 - val_loss: 0.4921 - val_categorical_accuracy: 0.8600 - lr: 0.0034\n",
      "Epoch 27/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4103 - categorical_accuracy: 0.8567\n",
      "Epoch 00027: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00027-0.41032-0.85671-0.47248-0.83000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.4103 - categorical_accuracy: 0.8567 - val_loss: 0.4725 - val_categorical_accuracy: 0.8300 - lr: 0.0034\n",
      "Epoch 28/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3757 - categorical_accuracy: 0.8763\n",
      "Epoch 00028: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00028-0.37572-0.87632-0.47055-0.82000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.3757 - categorical_accuracy: 0.8763 - val_loss: 0.4705 - val_categorical_accuracy: 0.8200 - lr: 0.0034\n",
      "Epoch 29/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3383 - categorical_accuracy: 0.9020\n",
      "Epoch 00029: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00029-0.33827-0.90196-0.46894-0.85000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.3383 - categorical_accuracy: 0.9020 - val_loss: 0.4689 - val_categorical_accuracy: 0.8500 - lr: 0.0034\n",
      "Epoch 30/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3214 - categorical_accuracy: 0.9065\n",
      "Epoch 00030: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00030-0.32138-0.90649-0.42557-0.80000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.3214 - categorical_accuracy: 0.9065 - val_loss: 0.4256 - val_categorical_accuracy: 0.8000 - lr: 0.0034\n",
      "Epoch 31/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2841 - categorical_accuracy: 0.9216\n",
      "Epoch 00031: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00031-0.28414-0.92157-0.39154-0.88000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.2841 - categorical_accuracy: 0.9216 - val_loss: 0.3915 - val_categorical_accuracy: 0.8800 - lr: 0.0034\n",
      "Epoch 32/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2794 - categorical_accuracy: 0.9216\n",
      "Epoch 00032: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00032-0.27943-0.92157-0.40754-0.85000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.2794 - categorical_accuracy: 0.9216 - val_loss: 0.4075 - val_categorical_accuracy: 0.8500 - lr: 0.0034\n",
      "Epoch 33/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2494 - categorical_accuracy: 0.9306\n",
      "Epoch 00033: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00033-0.24941-0.93062-0.49531-0.80000.h5\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.002401000028476119.\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.2494 - categorical_accuracy: 0.9306 - val_loss: 0.4953 - val_categorical_accuracy: 0.8000 - lr: 0.0034\n",
      "Epoch 34/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2155 - categorical_accuracy: 0.9563\n",
      "Epoch 00034: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00034-0.21553-0.95626-0.42047-0.85000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.2155 - categorical_accuracy: 0.9563 - val_loss: 0.4205 - val_categorical_accuracy: 0.8500 - lr: 0.0024\n",
      "Epoch 35/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2127 - categorical_accuracy: 0.9548\n",
      "Epoch 00035: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00035-0.21268-0.95475-0.40726-0.84000.h5\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0016807000851258634.\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.2127 - categorical_accuracy: 0.9548 - val_loss: 0.4073 - val_categorical_accuracy: 0.8400 - lr: 0.0024\n",
      "Epoch 36/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1992 - categorical_accuracy: 0.9593\n",
      "Epoch 00036: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00036-0.19915-0.95928-0.43064-0.84000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1992 - categorical_accuracy: 0.9593 - val_loss: 0.4306 - val_categorical_accuracy: 0.8400 - lr: 0.0017\n",
      "Epoch 37/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1946 - categorical_accuracy: 0.9548\n",
      "Epoch 00037: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00037-0.19456-0.95475-0.43780-0.83000.h5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0011764900758862494.\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1946 - categorical_accuracy: 0.9548 - val_loss: 0.4378 - val_categorical_accuracy: 0.8300 - lr: 0.0017\n",
      "Epoch 38/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1887 - categorical_accuracy: 0.9502\n",
      "Epoch 00038: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00038-0.18868-0.95023-0.36819-0.84000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1887 - categorical_accuracy: 0.9502 - val_loss: 0.3682 - val_categorical_accuracy: 0.8400 - lr: 0.0012\n",
      "Epoch 39/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1850 - categorical_accuracy: 0.9593\n",
      "Epoch 00039: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00039-0.18499-0.95928-0.42718-0.83000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1850 - categorical_accuracy: 0.9593 - val_loss: 0.4272 - val_categorical_accuracy: 0.8300 - lr: 0.0012\n",
      "Epoch 40/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1782 - categorical_accuracy: 0.9578\n",
      "Epoch 00040: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00040-0.17816-0.95777-0.34507-0.86000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1782 - categorical_accuracy: 0.9578 - val_loss: 0.3451 - val_categorical_accuracy: 0.8600 - lr: 0.0012\n",
      "Epoch 41/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1727 - categorical_accuracy: 0.9608\n",
      "Epoch 00041: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00041-0.17272-0.96078-0.47370-0.81000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1727 - categorical_accuracy: 0.9608 - val_loss: 0.4737 - val_categorical_accuracy: 0.8100 - lr: 0.0012\n",
      "Epoch 42/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1763 - categorical_accuracy: 0.9653\n",
      "Epoch 00042: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00042-0.17634-0.96531-0.37573-0.85000.h5\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0008235430694185197.\n",
      "39/39 [==============================] - 60s 2s/step - loss: 0.1763 - categorical_accuracy: 0.9653 - val_loss: 0.3757 - val_categorical_accuracy: 0.8500 - lr: 0.0012\n",
      "Epoch 43/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1644 - categorical_accuracy: 0.9698\n",
      "Epoch 00043: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00043-0.16437-0.96983-0.42578-0.83000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1644 - categorical_accuracy: 0.9698 - val_loss: 0.4258 - val_categorical_accuracy: 0.8300 - lr: 8.2354e-04\n",
      "Epoch 44/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1831 - categorical_accuracy: 0.9532\n",
      "Epoch 00044: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00044-0.18305-0.95324-0.42136-0.83000.h5\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005764801404438912.\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1831 - categorical_accuracy: 0.9532 - val_loss: 0.4214 - val_categorical_accuracy: 0.8300 - lr: 8.2354e-04\n",
      "Epoch 45/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1491 - categorical_accuracy: 0.9683\n",
      "Epoch 00045: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00045-0.14909-0.96833-0.53101-0.77000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1491 - categorical_accuracy: 0.9683 - val_loss: 0.5310 - val_categorical_accuracy: 0.7700 - lr: 5.7648e-04\n",
      "Epoch 46/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1708 - categorical_accuracy: 0.9578\n",
      "Epoch 00046: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00046-0.17081-0.95777-0.35369-0.85000.h5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0004035360820125788.\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1708 - categorical_accuracy: 0.9578 - val_loss: 0.3537 - val_categorical_accuracy: 0.8500 - lr: 5.7648e-04\n",
      "Epoch 47/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1413 - categorical_accuracy: 0.9729\n",
      "Epoch 00047: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00047-0.14126-0.97285-0.46553-0.80000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1413 - categorical_accuracy: 0.9729 - val_loss: 0.4655 - val_categorical_accuracy: 0.8000 - lr: 4.0354e-04\n",
      "Epoch 48/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1514 - categorical_accuracy: 0.9668\n",
      "Epoch 00048: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00048-0.15139-0.96682-0.45421-0.84000.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002824752533342689.\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1514 - categorical_accuracy: 0.9668 - val_loss: 0.4542 - val_categorical_accuracy: 0.8400 - lr: 4.0354e-04\n",
      "Epoch 49/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1607 - categorical_accuracy: 0.9698\n",
      "Epoch 00049: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00049-0.16066-0.96983-0.42373-0.83000.h5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.1607 - categorical_accuracy: 0.9698 - val_loss: 0.4237 - val_categorical_accuracy: 0.8300 - lr: 2.8248e-04\n",
      "Epoch 50/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1466 - categorical_accuracy: 0.9729\n",
      "Epoch 00050: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00050-0.14656-0.97285-0.44879-0.82000.h5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001977326814085245.\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1466 - categorical_accuracy: 0.9729 - val_loss: 0.4488 - val_categorical_accuracy: 0.8200 - lr: 2.8248e-04\n",
      "Epoch 51/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1451 - categorical_accuracy: 0.9683\n",
      "Epoch 00051: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00051-0.14506-0.96833-0.41781-0.85000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1451 - categorical_accuracy: 0.9683 - val_loss: 0.4178 - val_categorical_accuracy: 0.8500 - lr: 1.9773e-04\n",
      "Epoch 52/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1571 - categorical_accuracy: 0.9683\n",
      "Epoch 00052: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00052-0.15710-0.96833-0.32120-0.85000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1571 - categorical_accuracy: 0.9683 - val_loss: 0.3212 - val_categorical_accuracy: 0.8500 - lr: 1.9773e-04\n",
      "Epoch 53/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1451 - categorical_accuracy: 0.9774\n",
      "Epoch 00053: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00053-0.14505-0.97738-0.43308-0.84000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1451 - categorical_accuracy: 0.9774 - val_loss: 0.4331 - val_categorical_accuracy: 0.8400 - lr: 1.9773e-04\n",
      "Epoch 54/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1458 - categorical_accuracy: 0.9683\n",
      "Epoch 00054: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00054-0.14584-0.96833-0.42141-0.84000.h5\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00013841287291143088.\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1458 - categorical_accuracy: 0.9683 - val_loss: 0.4214 - val_categorical_accuracy: 0.8400 - lr: 1.9773e-04\n",
      "Epoch 55/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1536 - categorical_accuracy: 0.9683\n",
      "Epoch 00055: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00055-0.15359-0.96833-0.42540-0.84000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1536 - categorical_accuracy: 0.9683 - val_loss: 0.4254 - val_categorical_accuracy: 0.8400 - lr: 1.3841e-04\n",
      "Epoch 56/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1336 - categorical_accuracy: 0.9683\n",
      "Epoch 00056: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00056-0.13363-0.96833-0.54354-0.80000.h5\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.68890090007335e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1336 - categorical_accuracy: 0.9683 - val_loss: 0.5435 - val_categorical_accuracy: 0.8000 - lr: 1.3841e-04\n",
      "Epoch 57/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1484 - categorical_accuracy: 0.9683\n",
      "Epoch 00057: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00057-0.14844-0.96833-0.42097-0.84000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.1484 - categorical_accuracy: 0.9683 - val_loss: 0.4210 - val_categorical_accuracy: 0.8400 - lr: 9.6889e-05\n",
      "Epoch 58/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1554 - categorical_accuracy: 0.9653\n",
      "Epoch 00058: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00058-0.15543-0.96531-0.33033-0.86000.h5\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.782230630051344e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1554 - categorical_accuracy: 0.9653 - val_loss: 0.3303 - val_categorical_accuracy: 0.8600 - lr: 9.6889e-05\n",
      "Epoch 59/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1402 - categorical_accuracy: 0.9653\n",
      "Epoch 00059: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00059-0.14021-0.96531-0.55425-0.80000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1402 - categorical_accuracy: 0.9653 - val_loss: 0.5543 - val_categorical_accuracy: 0.8000 - lr: 6.7822e-05\n",
      "Epoch 60/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1491 - categorical_accuracy: 0.9593\n",
      "Epoch 00060: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00060-0.14910-0.95928-0.36618-0.86000.h5\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 4.7475615428993474e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1491 - categorical_accuracy: 0.9593 - val_loss: 0.3662 - val_categorical_accuracy: 0.8600 - lr: 6.7822e-05\n",
      "Epoch 61/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1502 - categorical_accuracy: 0.9668\n",
      "Epoch 00061: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00061-0.15017-0.96682-0.42576-0.84000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1502 - categorical_accuracy: 0.9668 - val_loss: 0.4258 - val_categorical_accuracy: 0.8400 - lr: 4.7476e-05\n",
      "Epoch 62/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1519 - categorical_accuracy: 0.9653\n",
      "Epoch 00062: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00062-0.15192-0.96531-0.45483-0.83000.h5\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 3.3232931309612467e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1519 - categorical_accuracy: 0.9653 - val_loss: 0.4548 - val_categorical_accuracy: 0.8300 - lr: 4.7476e-05\n",
      "Epoch 63/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1447 - categorical_accuracy: 0.9668\n",
      "Epoch 00063: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00063-0.14469-0.96682-0.43501-0.84000.h5\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.1447 - categorical_accuracy: 0.9668 - val_loss: 0.4350 - val_categorical_accuracy: 0.8400 - lr: 3.3233e-05\n",
      "Epoch 64/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1616 - categorical_accuracy: 0.9517\n",
      "Epoch 00064: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00064-0.16158-0.95173-0.41245-0.83000.h5\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.3263052935362793e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1616 - categorical_accuracy: 0.9517 - val_loss: 0.4124 - val_categorical_accuracy: 0.8300 - lr: 3.3233e-05\n",
      "Epoch 65/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1477 - categorical_accuracy: 0.9638\n",
      "Epoch 00065: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00065-0.14767-0.96380-0.37058-0.86000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1477 - categorical_accuracy: 0.9638 - val_loss: 0.3706 - val_categorical_accuracy: 0.8600 - lr: 2.3263e-05\n",
      "Epoch 66/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1585 - categorical_accuracy: 0.9638\n",
      "Epoch 00066: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00066-0.15850-0.96380-0.41326-0.84000.h5\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.6284137564070986e-05.\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1585 - categorical_accuracy: 0.9638 - val_loss: 0.4133 - val_categorical_accuracy: 0.8400 - lr: 2.3263e-05\n",
      "Epoch 67/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1597 - categorical_accuracy: 0.9653\n",
      "Epoch 00067: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00067-0.15969-0.96531-0.42589-0.83000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1597 - categorical_accuracy: 0.9653 - val_loss: 0.4259 - val_categorical_accuracy: 0.8300 - lr: 1.6284e-05\n",
      "Epoch 68/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1577 - categorical_accuracy: 0.9668\n",
      "Epoch 00068: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00068-0.15770-0.96682-0.36741-0.85000.h5\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.1398895912861917e-05.\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.1577 - categorical_accuracy: 0.9668 - val_loss: 0.3674 - val_categorical_accuracy: 0.8500 - lr: 1.6284e-05\n",
      "Epoch 69/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1710 - categorical_accuracy: 0.9578\n",
      "Epoch 00069: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00069-0.17105-0.95777-0.31289-0.87000.h5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1710 - categorical_accuracy: 0.9578 - val_loss: 0.3129 - val_categorical_accuracy: 0.8700 - lr: 1.1399e-05\n",
      "Epoch 70/70\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1614 - categorical_accuracy: 0.9668\n",
      "Epoch 00070: saving model to Conv_GRU_2022-02-1413_08_27.704058/model-00070-0.16143-0.96682-0.55231-0.79000.h5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1614 - categorical_accuracy: 0.9668 - val_loss: 0.5523 - val_categorical_accuracy: 0.7900 - lr: 1.1399e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe007cbdd50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_Gesture_Recognition_Case_Study_Final_Final (5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
